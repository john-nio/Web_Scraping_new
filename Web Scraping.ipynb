{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "69a953c4-ef69-4229-98b3-858033d711ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # To interact with the operating System \n",
    "import requests # To access the website. If connected succesfully the status code will be 200\n",
    "import re # To import regression Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "055b0d3e-3214-4ff7-9b44-1f22d532b93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "ff58f67f-ab96-4398-bdad-8ecd9ffe0bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page():\n",
    "    global url\n",
    "    url = input(\"Enter URL of the article you want to scrap: \")\n",
    "    print(f\"You entered: {url}\")\n",
    "    \n",
    "    #Handling Errors\n",
    "    #r' is used to escape backslash. Or double backslash can be used\n",
    "    #s? - To make sure to it accept the url either with s or without it\n",
    "    if not re.match(r'https?://medium.com/',url):\n",
    "        print('Please enter a valid website, or make sure it is a medium article')\n",
    "        sys.exit(1)\n",
    "    try:\n",
    "        res = requests.get(url)\n",
    "        res.raise_for_status()  # Raises an HTTPError for bad responses (4xx and 5xx)\n",
    "        print(\"Request successful.\")\n",
    "       \n",
    "        \n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        return soup\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def clean(text):\n",
    "    rep = {\"<br>\": \"\\n\", \"<br/>\": \"\\n\", \"<li>\": \"\\n\"}\n",
    "    \n",
    "    # Escape the dictionary keys for use in regex\n",
    "    rep = dict((re.escape(k), v) for k, v in rep.items())\n",
    "    \n",
    "    # Create a regex pattern to match any of the keys\n",
    "    pattern = re.compile(\"|\".join(rep.keys()))\n",
    "    \n",
    "    # Substitute tags with their corresponding replacements\n",
    "    text = pattern.sub(lambda m: rep[re.escape(m.group(0))], text)\n",
    "    \n",
    "    # Remove any remaining HTML tags\n",
    "    text = re.sub(r'<[^>]*>', '', text)  # Changed pattern to handle all tags\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "3748c7c8-d342-4ce6-8b26-46b2c1bf5e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_text(soup):\n",
    "\ttext = f'url: {url}\\n\\n'\n",
    "\tpara_text = soup.find_all('p')\n",
    "\tprint(f\"Number of paragraphs found:  {len(para_text)}\")\n",
    "\tfor para in para_text:\n",
    "\t\ttext += f\"{para.text}\\n\\n\"\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "79ad1f35-8d85-486b-b150-2a3637f9fdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(text):\n",
    "    # Ensure the global variable 'url' is defined\n",
    "    global url\n",
    "    \n",
    "    # Define the directory path\n",
    "    directory = './scraped_articles'\n",
    "    \n",
    "    # Create directory if it does not exist\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)\n",
    "    \n",
    "    # Generate a file name based on the URL\n",
    "    name = url.split(\"/\")[-1]\n",
    "    fname = f'{directory}/{name}.txt'\n",
    "    \n",
    "    # Debug information\n",
    "    print(f\"Saving file to: {fname}\")\n",
    "    \n",
    "    # Write the text to the file\n",
    "    with open(fname, 'w') as file:\n",
    "        file.write(text)\n",
    "    \n",
    "    print(f'File saved in directory {fname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "1e1b28f3-8480-4e56-b7a0-01a9bb8d2ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter URL of the article you want to scrap:  www.google.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You entered: www.google.com\n",
      "Please enter a valid website, or make sure it is a medium article\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        soup = get_page()  # Get the BeautifulSoup object\n",
    "        text = collect_text(soup)  # Collect the text\n",
    "        save_file(text)  # Save the text to a file\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "6d2c74ee-65e6-4ff2-a2fa-76f219221821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(text):\n",
    "    if not os.path.exists('./scraped_articles'):\n",
    "        os.mkdir('./scraped_articles')\n",
    "    name = url.split(\"/\")[-1]\n",
    "    print(name)\n",
    "    fname = f'scraped_articles/{name}.txt'    \n",
    "    with open(fname, 'w') as file:\n",
    "        file.write(text)\n",
    "    print(f'File saved in directory {fname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04668005-bb23-40b9-9b77-fbb27d8d1914",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
